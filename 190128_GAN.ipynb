{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190128_GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "QnIzu3uWTBc3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2016년도 관심 분야\n",
        "* 비지도 학습 방법"
      ]
    },
    {
      "metadata": {
        "id": "Ew6BDo7oUHcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "9ca64c35-b151-4bc3-cb68-5d56b0638f63"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nDA1Rbm6UZiY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 01 기본 옵션 설정"
      ]
    },
    {
      "metadata": {
        "id": "AO9jNo4UUtAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_epoch = 100 # epoch 수 설정 (왕복 횟수)\n",
        "batch_size = 100 # 배치 사이즈\n",
        "learning_rate = 0.0002 #학습률 (매우 느리게.)\n",
        "\n",
        "# 신경망 레이어 구성 옵션\n",
        "## 은닉층의 노드 수 결정\n",
        "n_hidden = 256\n",
        "# 입력층 갯수\n",
        "n_input = 28*28 \n",
        "# 페이크 생성기의 입력값으로 사용할 노이즈(데이터)의 크기(갯수)\n",
        "n_noise = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "50rXk_2KVaSY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 02. 신경망 모델 구성\n",
        "* 노이즈를 이용한 데이터 생성\n",
        "* 비지도 학습으로 Y가 없음"
      ]
    },
    {
      "metadata": {
        "id": "66cimTQ8VkFR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 판별기에 들어갈 입력\n",
        "X = tf.placeholder(tf.float32, [None, n_input])\n",
        "\n",
        "## 페이크 생성기에 들어갈 입력\n",
        "Z = tf.placeholder(tf.float32, [None, n_noise])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtY-_iHnV9bP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 생성기의 가중치(W)와 바이어스(b) "
      ]
    },
    {
      "metadata": {
        "id": "TYef09mDWXYX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 128*256 개의 W1, \n",
        "# 128 개의 b1\n",
        "\n",
        "#G_W1 = tf.placeholder(tf.random_normal([n_noise,n_hidden], stddev=0.01))\n",
        "#G_b1 = tf.Variable(tf.zeros([n_hidden])) # 256\n",
        "\n",
        "#G_W2 = tf.placeholder(tf.random_normal([n_hidden,n_input], stddev=0.01))\n",
        "#G_b2 = tf.Variable(tf.zeros([n_input])) # 128\n",
        "\n",
        "\n",
        "G_W1 = tf.Variable(tf.random_normal([128,256], stddev=0.01))\n",
        "G_b1 = tf.Variable(tf.zeros([256])) # 256\n",
        "\n",
        "G_W2 = tf.Variable(tf.random_normal([256,784], stddev=0.01))\n",
        "G_b2 = tf.Variable(tf.zeros([784])) # 128\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bOKLG4H9cxc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 구분자의 신경망 변수 선언(W, b)\n",
        "* 은닉층이 들어가면 선형 뿐만아니라 비선형 문제도 풀 수 있다.\n",
        "* 은닉층이 없으면 선형문제만 풀 수 있다."
      ]
    },
    {
      "metadata": {
        "id": "2lSAM4xIdEHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D_W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))\n",
        "D_b1 = tf.Variable(tf.zeros([n_hidden])) # 256\n",
        "\n",
        "\n",
        "D_W2 = tf.Variable(tf.random_normal([256, 1], stddev=0.01))\n",
        "D_b2 = tf.Variable(tf.zeros([1])) # 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xHlOw0zNeAdI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2-2 생성기 신경망 구성"
      ]
    },
    {
      "metadata": {
        "id": "dEh4B6iifmZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(noise_z):\n",
        "  hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)   # 은닉층 거친 결과\n",
        "  output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2) # 2번째 거친 결과(128)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Emou2WPjoiaj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2-3 구분자(D) 신경망 구성"
      ]
    },
    {
      "metadata": {
        "id": "FCTkEfo0okEP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator(inputs):\n",
        "  hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
        "  output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2CqBI_ComIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def discriminator(inputs):\n",
        "#   hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)   # 은닉층 거친 결과\n",
        "#   output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2) # 2번째 거친 결과(128)\n",
        "#   return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KVXWP-MXond_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2-4 위의 noise_z를 발생시킬 노이즈 생성 함수"
      ]
    },
    {
      "metadata": {
        "id": "j8iSda6WopBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_noise(batch_size, n_noise):\n",
        "  return np.random.normal(size=(batch_size, n_noise))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nN7IzyjrorQm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  노이즈를 이용해서 랜덤한 이미지를 생성.\n",
        "G = generator(Z)\n",
        "\n",
        "# 노이즈를 이용해서 생성한 이미지(정보)를 구분자에 넣어서 결과(판별값0~1)을 본다.\n",
        "D_fake = discriminator(G)\n",
        "\n",
        "# 진짜 이미지를 이용해서 판별한 값을 구한다.\n",
        "D_real = discriminator(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GeLN6PO9oslU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. \n",
        "* loss \n",
        "* optimizer\n",
        "* 진짜 이미지를 넣었을 때, 최대값(tf.log(D_real))\n",
        "* 가짜 이미지를 넣었을 때, 최대값 tf.log(1-D_fake)"
      ]
    },
    {
      "metadata": {
        "id": "TojcVFlfptQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1-D_fake))\n",
        "## 생성자의 loss\n",
        "loss_G = tf.reduce_mean(tf.log(D_fake))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kY4YP1IwpxTm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D_var_list = [D_W1, D_b1, D_W2, D_b2]  #   구분자의 변수들\n",
        "G_var_list = [G_W1, G_b1, G_W2, G_b2]  #   생성자의 변수들"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvZmKL51py3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GAN 논문의 수식에 따르면 loss 를 극대화 해야하지만, minimize 하는 최적화 함수를 사용하기 때문에\n",
        "# 최적화 하려는 loss_D 와 loss_G 에 음수 부호를 붙여줍니다.\n",
        "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
        "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qq7ZUkeyp0GA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "total_batch = int(mnist.train.num_examples/batch_size)\n",
        "loss_val_D, loss_val_G = 0, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "697TIutsp1BF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tlq8BPpp2ey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1871
        },
        "outputId": "633683c7-95ac-4c62-da6d-4f2ebfb6cad7"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(total_epoch):\n",
        "  for i in range(total_batch):\n",
        "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "    noise = get_noise(batch_size, n_noise)\n",
        "    # 판별기와 생성기 신경망을 각각 학습시킵니다.\n",
        "    _, loss_val_D = sess.run([train_D, loss_D],\n",
        "    feed_dict={X: batch_xs, Z: noise})\n",
        "    _, loss_val_G = sess.run([train_G, loss_G],\n",
        "    feed_dict={Z: noise})\n",
        "  print('Epoch:', '%04d' % epoch,\n",
        "        'D loss: {:.4}'.format(loss_val_D),\n",
        "        'G loss: {:.4}'.format(loss_val_G))\n",
        "\n",
        "    #########\n",
        "    # 학습이 되어가는 모습을 보기 위해 주기적으로 이미지를 생성하여 저장\n",
        "    ######\n",
        "  if epoch == 0 or (epoch + 1) % 10 == 0:\n",
        "      sample_size = 10\n",
        "      noise = get_noise(sample_size, n_noise)\n",
        "      samples = sess.run(G, feed_dict={Z: noise})\n",
        "      fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))\n",
        "      \n",
        "      for i in range(sample_size):\n",
        "        ax[i].set_axis_off()\n",
        "        ax[i].imshow(np.reshape(samples[i], (28, 28)))\n",
        "      plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
        "      plt.close(fig)\n",
        "print('최적화 완료!')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0000 D loss: -0.4419 G loss: -2.123\n",
            "Epoch: 0001 D loss: -0.5463 G loss: -2.061\n",
            "Epoch: 0002 D loss: -0.2765 G loss: -2.382\n",
            "Epoch: 0003 D loss: -0.6085 G loss: -1.385\n",
            "Epoch: 0004 D loss: -0.409 G loss: -1.917\n",
            "Epoch: 0005 D loss: -0.1989 G loss: -2.478\n",
            "Epoch: 0006 D loss: -0.4351 G loss: -1.949\n",
            "Epoch: 0007 D loss: -0.2335 G loss: -2.938\n",
            "Epoch: 0008 D loss: -0.2933 G loss: -2.675\n",
            "Epoch: 0009 D loss: -0.3514 G loss: -2.558\n",
            "Epoch: 0010 D loss: -0.5002 G loss: -2.551\n",
            "Epoch: 0011 D loss: -0.4325 G loss: -2.124\n",
            "Epoch: 0012 D loss: -0.4633 G loss: -2.083\n",
            "Epoch: 0013 D loss: -0.4236 G loss: -2.46\n",
            "Epoch: 0014 D loss: -0.5581 G loss: -1.909\n",
            "Epoch: 0015 D loss: -0.6003 G loss: -1.842\n",
            "Epoch: 0016 D loss: -0.5391 G loss: -1.778\n",
            "Epoch: 0017 D loss: -0.5661 G loss: -2.032\n",
            "Epoch: 0018 D loss: -0.7108 G loss: -1.825\n",
            "Epoch: 0019 D loss: -0.6687 G loss: -2.032\n",
            "Epoch: 0020 D loss: -0.7094 G loss: -1.841\n",
            "Epoch: 0021 D loss: -0.5613 G loss: -2.177\n",
            "Epoch: 0022 D loss: -0.6684 G loss: -2.009\n",
            "Epoch: 0023 D loss: -0.6049 G loss: -2.157\n",
            "Epoch: 0024 D loss: -0.7104 G loss: -2.031\n",
            "Epoch: 0025 D loss: -0.6508 G loss: -2.077\n",
            "Epoch: 0026 D loss: -0.7774 G loss: -1.844\n",
            "Epoch: 0027 D loss: -0.6317 G loss: -1.983\n",
            "Epoch: 0028 D loss: -0.8342 G loss: -1.772\n",
            "Epoch: 0029 D loss: -0.7574 G loss: -1.883\n",
            "Epoch: 0030 D loss: -0.7577 G loss: -1.706\n",
            "Epoch: 0031 D loss: -0.6321 G loss: -2.129\n",
            "Epoch: 0032 D loss: -0.9866 G loss: -1.804\n",
            "Epoch: 0033 D loss: -0.8648 G loss: -1.673\n",
            "Epoch: 0034 D loss: -0.696 G loss: -1.907\n",
            "Epoch: 0035 D loss: -0.8028 G loss: -1.864\n",
            "Epoch: 0036 D loss: -0.84 G loss: -1.82\n",
            "Epoch: 0037 D loss: -0.7092 G loss: -2.032\n",
            "Epoch: 0038 D loss: -0.8243 G loss: -1.693\n",
            "Epoch: 0039 D loss: -0.7922 G loss: -1.833\n",
            "Epoch: 0040 D loss: -0.636 G loss: -1.764\n",
            "Epoch: 0041 D loss: -0.8576 G loss: -1.756\n",
            "Epoch: 0042 D loss: -0.9621 G loss: -1.596\n",
            "Epoch: 0043 D loss: -0.7816 G loss: -1.766\n",
            "Epoch: 0044 D loss: -0.7711 G loss: -1.905\n",
            "Epoch: 0045 D loss: -0.7625 G loss: -1.69\n",
            "Epoch: 0046 D loss: -0.9426 G loss: -1.575\n",
            "Epoch: 0047 D loss: -0.9457 G loss: -1.668\n",
            "Epoch: 0048 D loss: -0.9052 G loss: -1.576\n",
            "Epoch: 0049 D loss: -1.05 G loss: -1.506\n",
            "Epoch: 0050 D loss: -0.9854 G loss: -1.736\n",
            "Epoch: 0051 D loss: -0.7917 G loss: -1.623\n",
            "Epoch: 0052 D loss: -0.8184 G loss: -1.535\n",
            "Epoch: 0053 D loss: -0.9992 G loss: -1.687\n",
            "Epoch: 0054 D loss: -0.8653 G loss: -1.62\n",
            "Epoch: 0055 D loss: -0.8188 G loss: -1.502\n",
            "Epoch: 0056 D loss: -0.8403 G loss: -1.504\n",
            "Epoch: 0057 D loss: -0.9668 G loss: -1.499\n",
            "Epoch: 0058 D loss: -0.9514 G loss: -1.546\n",
            "Epoch: 0059 D loss: -0.8881 G loss: -1.404\n",
            "Epoch: 0060 D loss: -0.7642 G loss: -1.697\n",
            "Epoch: 0061 D loss: -1.036 G loss: -1.34\n",
            "Epoch: 0062 D loss: -0.941 G loss: -1.593\n",
            "Epoch: 0063 D loss: -0.9696 G loss: -1.521\n",
            "Epoch: 0064 D loss: -1.054 G loss: -1.453\n",
            "Epoch: 0065 D loss: -0.8888 G loss: -1.495\n",
            "Epoch: 0066 D loss: -0.8272 G loss: -1.281\n",
            "Epoch: 0067 D loss: -0.8576 G loss: -1.634\n",
            "Epoch: 0068 D loss: -1.036 G loss: -1.279\n",
            "Epoch: 0069 D loss: -0.9732 G loss: -1.424\n",
            "Epoch: 0070 D loss: -0.9567 G loss: -1.403\n",
            "Epoch: 0071 D loss: -0.8439 G loss: -1.495\n",
            "Epoch: 0072 D loss: -0.9587 G loss: -1.477\n",
            "Epoch: 0073 D loss: -1.01 G loss: -1.329\n",
            "Epoch: 0074 D loss: -0.8428 G loss: -1.568\n",
            "Epoch: 0075 D loss: -1.022 G loss: -1.26\n",
            "Epoch: 0076 D loss: -0.9502 G loss: -1.529\n",
            "Epoch: 0077 D loss: -0.957 G loss: -1.495\n",
            "Epoch: 0078 D loss: -0.8728 G loss: -1.484\n",
            "Epoch: 0079 D loss: -0.8133 G loss: -1.556\n",
            "Epoch: 0080 D loss: -0.9727 G loss: -1.436\n",
            "Epoch: 0081 D loss: -0.9476 G loss: -1.553\n",
            "Epoch: 0082 D loss: -0.8722 G loss: -1.524\n",
            "Epoch: 0083 D loss: -1.027 G loss: -1.433\n",
            "Epoch: 0084 D loss: -0.9412 G loss: -1.469\n",
            "Epoch: 0085 D loss: -0.9039 G loss: -1.508\n",
            "Epoch: 0086 D loss: -0.9665 G loss: -1.371\n",
            "Epoch: 0087 D loss: -0.8591 G loss: -1.567\n",
            "Epoch: 0088 D loss: -0.9305 G loss: -1.58\n",
            "Epoch: 0089 D loss: -1.001 G loss: -1.479\n",
            "Epoch: 0090 D loss: -1.126 G loss: -1.472\n",
            "Epoch: 0091 D loss: -1.011 G loss: -1.484\n",
            "Epoch: 0092 D loss: -0.9537 G loss: -1.474\n",
            "Epoch: 0093 D loss: -1.047 G loss: -1.367\n",
            "Epoch: 0094 D loss: -1.037 G loss: -1.38\n",
            "Epoch: 0095 D loss: -0.8416 G loss: -1.509\n",
            "Epoch: 0096 D loss: -0.9243 G loss: -1.597\n",
            "Epoch: 0097 D loss: -0.8509 G loss: -1.395\n",
            "Epoch: 0098 D loss: -0.9023 G loss: -1.529\n",
            "Epoch: 0099 D loss: -0.811 G loss: -1.492\n",
            "최적화 완료!\n",
            "CPU times: user 23min 33s, sys: 1min 12s, total: 24min 46s\n",
            "Wall time: 14min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NHepzU7ap39U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}